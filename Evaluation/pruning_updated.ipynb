{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8899d2-234c-4032-ac73-8a7c870c0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import os\n",
    "from model.networks import Generator\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9bd42d-9f8f-4c87-a440-434c1a1df4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    generator = Generator(checkpoint=checkpoint_path).to(device)\n",
    "    return generator\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    zero_count = 0\n",
    "    total_count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        zero_count += torch.sum(param == 0).item()\n",
    "        total_count += param.numel()\n",
    "    return zero_count, total_count\n",
    "\n",
    "def prune_model(model, amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def save_model(model, path):\n",
    "    state_dict = {'G': model.state_dict()}  # Ensure the 'G' key is included\n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "def print_model_size(model):\n",
    "    param_size = sum(param.numel() for param in model.parameters())\n",
    "    buffer_size = sum(buffer.numel() for buffer in model.buffers())\n",
    "    size_all_mb = (param_size + buffer_size) * 4 / 1024**2\n",
    "    print(f'Model size in memory: {size_all_mb:.3f} MB')\n",
    "\n",
    "def test_model(model):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 5, 256, 256).to(device)  # assuming input shape\n",
    "        dummy_mask = torch.ones(1, 1, 256, 256).to(device)\n",
    "        try:\n",
    "            output = model(dummy_input, dummy_mask)\n",
    "            print(\"Model forward pass successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model forward pass failed: {e}\")\n",
    "\n",
    "def get_file_size(path):\n",
    "    size = os.path.getsize(path) / 1024**2  # in MB\n",
    "    return size\n",
    "\n",
    "def main(prune_amounts, initial_checkpoint_path, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    model = load_model(initial_checkpoint_path)\n",
    "\n",
    "    for i, amount in enumerate(prune_amounts):\n",
    "        zero_count_before, total_count = count_zero_weights(model)\n",
    "\n",
    "        pruned_model = prune_model(copy.deepcopy(model), amount)\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"pruned_model_{int(amount*100)}.pth\")\n",
    "        save_model(pruned_model, save_path)\n",
    "        print(f\"Saved pruned model with {amount*100}% pruning at: {save_path}\")\n",
    "\n",
    "        test_model(pruned_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba36d48-30ea-4b83-9d55-2c23cebc2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned model with 0.0% pruning at: C:/Users/tuant/Downloads/AI Thesis/FINAL_APP_AND_ANALYSIS/optimized_models\\pruned_model_0.pth\n",
      "Model forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    initial_checkpoint_path = \"C:/Users/tuant/Downloads/AI Thesis/FINAL_APP_AND_ANALYSIS/optimized_models/states_pt_places2.pth\"\n",
    "    save_dir = \"C:/Users/tuant/Downloads/AI Thesis/FINAL_APP_AND_ANALYSIS/optimized_models\"\n",
    "    prune_amounts = [0.00]\n",
    "    # for i in np.arange(0.01, 1.00, 0.01):\n",
    "    #     prune_amounts.append(i)\n",
    "\n",
    "    main(prune_amounts, initial_checkpoint_path, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec669d5-df95-4196-ae61-99545afbadc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bef29-b47b-4b9b-9853-9a4f8870f01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaf65d-c08f-41ab-9b46-49d10ebfd02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0de23-6fd8-4954-a69d-a9f43e43b5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
